<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Hadoop - ArchWiki</title>
<link rel="stylesheet" href="../ArchWikiOffline.css">
<meta name="ResourceLoaderDynamicStyles" content="">
<meta name="generator" content="MediaWiki 1.35.0">
<meta name="robots" content="noindex,follow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/opensearch_desc.php" title="ArchWiki (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://wiki.archlinux.org/api.php?action=rsd">
<link rel="license" href="http://www.gnu.org/copyleft/fdl.html">
<link rel="alternate" type="application/atom+xml" title="ArchWiki Atom feed" href="/index.php?title=Special:RecentChanges&amp;feed=atom">
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Hadoop rootpage-Hadoop skin-vector action-view skin-vector-legacy">
<div id="content" class="mw-body" role="main" style="margin: 0">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"></div>
	<div class="mw-indicators mw-body-content">
	</div>
	<h1 id="firstHeading" class="firstHeading" lang="en">Hadoop</h1>
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From ArchWiki</div>
		<div id="contentSub"></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr">
<div class="warningbox">The printable version is no longer supported and may have rendering errors. Please update your browser bookmarks and please use the default browser print function instead.</div>
<div class="mw-parser-output">
<div class="archwiki-template-meta-related-articles-start">
<p>Related articles</p>
<ul>
<li><a href="../en/Apache_Spark.html" title="Apache Spark">Apache Spark</a></li>
</ul>
</div>
<p><a rel="nofollow" class="external text" href="https://hadoop.apache.org">Apache Hadoop</a> is a framework for running applications on large cluster built of commodity hardware. The Hadoop framework transparently provides applications both reliability and data motion. Hadoop implements a computational paradigm named Map/Reduce, where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node in the cluster. In addition, it provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. Both MapReduce and the Hadoop Distributed File System are designed so that node failures are automatically handled by the framework. 
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading">
<input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr">
<h2 id="mw-toc-heading">Contents</h2>
<span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Installation"><span class="tocnumber">1</span> <span class="toctext">Installation</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Configuration"><span class="tocnumber">2</span> <span class="toctext">Configuration</span></a></li>
<li class="toclevel-1 tocsection-3">
<a href="#Single_Node_Setup"><span class="tocnumber">3</span> <span class="toctext">Single Node Setup</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Standalone_Operation"><span class="tocnumber">3.1</span> <span class="toctext">Standalone Operation</span></a></li>
<li class="toclevel-2 tocsection-5">
<a href="#Pseudo-Distributed_Operation"><span class="tocnumber">3.2</span> <span class="toctext">Pseudo-Distributed Operation</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#Set_up_passphraseless_ssh"><span class="tocnumber">3.2.1</span> <span class="toctext">Set up passphraseless ssh</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Execution"><span class="tocnumber">3.2.2</span> <span class="toctext">Execution</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

<h2><span class="mw-headline" id="Installation">Installation</span></h2>
<p><a href="../en/Help:Reading.html#Installation_of_packages" class="mw-redirect" title="Install">Install</a> the <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/hadoop/">hadoop</a></span><sup><small>AUR</small></sup> package.
</p>
<h2><span class="mw-headline" id="Configuration">Configuration</span></h2>
<p>By default, hadoop is already configured for pseudo-distributed operation. Some environment variables are set in <code>/etc/profile.d/hadoop.sh</code> with different values than traditional hadoop.
</p>
<table class="wikitable">

<tbody>
<tr>
<th>ENV
</th>
<th>Value
</th>
<th>Description
</th>
<th>Permission
</th>
</tr>
<tr>
<td>HADOOP_CONF_DIR
</td>
<td>
<code>/etc/hadoop</code>
</td>
<td>Where configuration files are stored.
</td>
<td>Read
</td>
</tr>
<tr>
<td>HADOOP_LOG_DIR
</td>
<td>
<code>/tmp/hadoop/log</code>
</td>
<td>Where log files are stored.
</td>
<td>Read and Write
</td>
</tr>
<tr>
<td>HADOOP_WORKERS
</td>
<td>
<code>/etc/hadoop/workers</code>
</td>
<td>File naming remote worker hosts.
</td>
<td>Read
</td>
</tr>
<tr>
<td>HADOOP_PID_DIR
</td>
<td>
<code>/tmp/hadoop/run</code>
</td>
<td>Where pid files are stored.
</td>
<td>Read and Write
</td>
</tr>
</tbody>
</table>
<p>You also should set up the following files correctly. 
</p>
<pre>/etc/hosts
/etc/hostname 
/etc/locale.conf
</pre>
<p>You need to tell hadoop your JAVA_HOME in <code>/etc/hadoop/hadoop-env.sh</code> because it doesn't assume the location where it's installed to in Arch Linux by itself:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">/etc/hadoop/hadoop-env.sh</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk/jre</pre>
<p>Check installation with:
</p>
<pre>hadoop version
</pre>
<p>The HADOOP_WORKERS option was previously called HADOOP_SLAVES. If you get warning message "WARNING: HADOOP_SLAVES has been replaced by HADOOP_WORKERS. Using value of HADOOP_SLAVES." Then replace <code>export HADOOP_SLAVES=/etc/hadoop/slaves</code> in <code>/etc/profile.d/hadoop.sh</code> with:
</p>
<pre>export HADOOP_WORKERS=/etc/hadoop/workers
</pre>
<h2><span class="mw-headline" id="Single_Node_Setup">Single Node Setup</span></h2>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> This section is based on the <a rel="nofollow" class="external text" href="http://hadoop.apache.org/docs/stable/">Hadoop Official Documentation</a> </div>
<h3><span class="mw-headline" id="Standalone_Operation">Standalone Operation</span></h3>
<p>By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.
</p>
<p>The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory.
</p>
<pre>$ HADOOP_CONF_DIR=/usr/lib/hadoop/orig_etc/hadoop/
$ mkdir input
$ cp /etc/hadoop/*.xml input
$ hadoop jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output 'dfs[a-z.]+'
$ cat output/*
</pre>
<h3><span class="mw-headline" id="Pseudo-Distributed_Operation">Pseudo-Distributed Operation</span></h3>
<p>Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.
</p>
<p>By default, Hadoop will run as the user root. You can change the user in <code>/etc/conf.d/hadoop</code>:
</p>
<pre>HADOOP_USERNAME="&lt;your user name&gt;"
</pre>
<h4><span class="mw-headline" id="Set_up_passphraseless_ssh">Set up passphraseless ssh</span></h4>
<p>Make sure you have <a href="../en/OpenSSH.html" class="mw-redirect" title="Sshd">sshd</a> <a href="../en/Systemd.html#Using_units" class="mw-redirect" title="Enabled">enabled</a>. Now check that you can connect to localhost without a passphrase:
</p>
<pre>$ ssh localhost
</pre>
<p>If you cannot ssh to localhost without a passphrase, execute the following commands:
</p>
<pre>$ ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
</pre>
<p>Also make sure this line is commented in <code>/etc/ssh/sshd_config</code>
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">/etc/ssh/sshd_config</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">#AuthorizedKeysFile .ssh/authorized_keys</pre>
<h4><span class="mw-headline" id="Execution">Execution</span></h4>
<p>Format a new distributed-filesystem:
</p>
<pre>$ hadoop namenode -format
</pre>
<p>Edit <code>/etc/hadoop/core-site.xml</code> and add below configuration:
</p>
<pre> &lt;configuration&gt;
   &lt;property&gt;
     &lt;name&gt;fs.defaultFS&lt;/name&gt;
     &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
   &lt;/property&gt;
 &lt;/configuration&gt;
</pre>
<p>Start the hadoop namenode:
</p>
<pre>$ hadoop namenode
</pre>
<p>You may access the web GUI via <a rel="nofollow" class="external free" href="http://localhost:9870/">http://localhost:9870/</a>
</p>
<div class="noprint archwiki-template-message">
<p><a href="../File:Tango-inaccurate.png" class="image"><img alt="Tango-inaccurate.png" src="../File:Tango-inaccurate.png" decoding="async" width="48" height="48"></a><b>The factual accuracy of this article or section is disputed.</b><a href="../File:Tango-inaccurate.png" class="image"><img alt="Tango-inaccurate.png" src="../File:Tango-inaccurate.png" decoding="async" width="48" height="48"></a></p>
<div>
<b>Reason:</b> Instructions for older versions of Hadoop (Discuss in <a rel="nofollow" class="external text" href="https://wiki.archlinux.org/index.php/Talk:Hadoop">Talk:Hadoop#</a>)</div>
</div>
<p><a href="../en/Systemd.html#Using_units" class="mw-redirect" title="Start">Start</a> the following hadoop systemd units: <code>hadoop-datanode</code>, <code>hadoop-jobtracker</code>, <code>hadoop-namenode</code>, <code>hadoop-secondarynamenode</code>, <code>hadoop-tasktracker</code>.
</p>
<p>The hadoop daemon log output is written to the <code>${HADOOP_LOG_DIR}</code> directory (defaults to <code>/var/log/hadoop</code>).
</p>
<p>Browse the web interface for the NameNode and the JobTracker; by default they are available at:
</p>
<ul>
<li>NameNode - <a rel="nofollow" class="external free" href="http://localhost:50070/">http://localhost:50070/</a>
</li>
<li>JobTracker - <a rel="nofollow" class="external free" href="http://localhost:50030/">http://localhost:50030/</a>
</li>
</ul>
<p>Copy the input files into the distributed filesystem:
</p>
<pre>$ hadoop fs -put /etc/hadoop input
</pre>
<p>Run some of the examples provided:
</p>
<pre>$ hadoop jar /usr/lib/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'
</pre>
<p>Examine the output files:
</p>
<p>Copy the output files from the distributed filesystem to the local filesystem and examine them:
</p>
<pre>$ hadoop fs -get output output
$ cat output/*
</pre>
<p>or
</p>
<p>View the output files on the distributed filesystem:
</p>
<pre>$ hadoop fs -cat output/*
</pre>
<p>When you're done, <a href="../en/Systemd.html#Using_units" class="mw-redirect" title="Stop">stop</a> the daemons <code>hadoop-datanode</code>, <code>hadoop-jobtracker</code>, <code>hadoop-namenode</code>, <code>hadoop-secondarynamenode</code>, <code>hadoop-tasktracker</code>.
</p>
</div>
</div>
<div id="catlinks" class="catlinks" data-mw="interface">
<div id="mw-normal-catlinks" class="mw-normal-catlinks">
<a href="../Special:Categories.html" title="Special:Categories">Categories</a>: <ul>
<li><a href="../en/Category:Distributed_computing.html" title="Category:Distributed computing">Distributed computing</a></li>
<li><a href="../en/Category:Apache.html" title="Category:Apache">Apache</a></li>
</ul>
</div>
<div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden category: <ul><li><a href="../en/Category:Pages_or_sections_flagged_with_Template:Accuracy.html" title="Category:Pages or sections flagged with Template:Accuracy">Pages or sections flagged with Template:Accuracy</a></li></ul>
</div>
</div>
	</div>
</div>

<footer id="footer" class="mw-footer" role="contentinfo" style="margin: 0">
	<ul id="footer-info">
		<li>Retrieved from "<a dir="ltr" href="https://wiki.archlinux.org/index.php?title=Hadoop&amp;oldid=635694">https://wiki.archlinux.org/index.php?title=Hadoop&amp;oldid=635694</a>"</li>
		<li id="footer-info-lastmod"> This page was last edited on 18 September 2020, at 14:36.</li>
		<li id="footer-info-copyright">Content is available under <a class="external" rel="nofollow" href="http://www.gnu.org/copyleft/fdl.html">GNU Free Documentation License 1.3 or later</a> unless otherwise noted.</li>
	<br>
</ul>
	<ul id="footer-places">
		<li id="footer-places-privacy"><a href="../en/ArchWiki:Privacy_policy.html" title="ArchWiki:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="../en/ArchWiki:About.html" title="ArchWiki:About">About ArchWiki</a></li>
		<li id="footer-places-disclaimer"><a href="../en/ArchWiki:General_disclaimer.html" title="ArchWiki:General disclaimer">Disclaimers</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico">
	</ul>
	<div style="clear: both;"></div>
</footer>



</body>
</html>
